# Model Quantization Papers

<ul>

                             

 <li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(1).pdf" style="text-decoration:none;">Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or -1</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(2).pdf" style="text-decoration:none;">Ternary weight networks</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(3).pdf" style="text-decoration:none;">Ternary Neural Networks with Fine-Grained Quantization</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(4).pdf" style="text-decoration:none;">BMXNet: An Open-Source Binary Neural Network Implementation Based on MXNet</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(5).pdf" style="text-decoration:none;">Compact Hash Code Learning with
Binary Deep Neural Network</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(6).pdf" style="text-decoration:none;">[DL] A Survey of FPGA-Based Neural Network Inference Accelerator</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(7).pdf" style="text-decoration:none;">BinaryRelax: A Relaxation Approach For Training Deep Neural Networks With Quantized Weights</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(8).pdf" style="text-decoration:none;"> Xcel-RAM: Accelerating Binary Neural Networks in High-Throughput SRAM Compute Arrays </a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(9).pdf" style="text-decoration:none;">Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices</a></li>
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(10).pdf" style="text-decoration:none;">Improved training of binary networks for human pose estimation and image recognition </a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(11).pdf" style="text-decoration:none;">Matrix and tensor decompositions for training binary neural networks</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(12).pdf" style="text-decoration:none;">Back to Simplicity: How to Train Accurate BNNs from Scratch?</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(13).pdf" style="text-decoration:none;">Vector Quantized Bayesian Neural Network Inference for Data Streams</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(14).pdf" style="text-decoration:none;">Bayesian Optimized 1-Bit CNNs</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(15).pdf" style="text-decoration:none;">How Does Batch Normalization Help Binary Training?</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(16).pdf" style="text-decoration:none;">RPR: Random Partition Relaxation for Training Binary and Ternary Weight Neural Networks</a></li>

  <li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(17).pdf" style="text-decoration:none;">Post-training Quantization with Multiple Points: Mixed Precision without Mixed Precision</a></li>   
  
<li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(18).pdf" style="text-decoration:none;">Training Binary Neural Networks using the Bayesian Learning Rule</a></li> 

  
<li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(19).pdf" style="text-decoration:none;">Phoenix: A Low-Precision Floating-Point Quantization Oriented Architecture for Convolutional Neural Networks</a></li> 

<li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(20).pdf" style="text-decoration:none;">IMAC: In-memory multi-bit Multiplication and Accumulation in 6T SRAM Array</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(21).pdf" style="text-decoration:none;">MuBiNN: Multi-Level Binarized Recurrent Neural Network for EEG signal Classification</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(22).pdf" style="text-decoration:none;">Binarized Graph Neural Network</a></li> 
 <li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(23).pdf" style="text-decoration:none;">CP-NAS: Child-Parent Neural Architecture Search for Binary Neural Networks</a></li> 
 

   <li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(24).pdf" style="text-decoration:none;">Distillation Guided Residual Learning for Binary Convolutional Neural Networks</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(25).pdf" style="text-decoration:none;">WrapNet: Neural Net Inference with Ultra-Low-Resolution Arithmetic</a></li>                              
 <li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(26).pdf" style="text-decoration:none;">Memory and Computation-Efficient Kernel SVM via Binary Embedding and Ternary Model Coefficients</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(27).pdf" style="text-decoration:none;">Compressing Deep Convolutional Neural Networks by Stacking Low-dimensional Binary Convolution Filters</a></li>
   
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(28).pdf" style="text-decoration:none;">Scalable Verification of Quantized Neural Networks (Technical Report)</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(29).pdf" style="text-decoration:none;">DMS: Differentiable Dimension Search For Binary Neural Networks </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(30).pdf" style="text-decoration:none;">Binary Neural Networks: A Survey</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Model-Quantization-Papers/blob/master/m(31).pdf" style="text-decoration:none;">Extremely Low Bit Neural Network: Squeeze the Last Bit Out with ADMM</a></li> 
    </ul>
  
  
